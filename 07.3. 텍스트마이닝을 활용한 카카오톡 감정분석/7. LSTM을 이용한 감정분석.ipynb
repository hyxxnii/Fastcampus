{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NSMC를 딥러닝으로 해보자!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 불러온 데이터를 품사 태그를 붙여서 토크나이징합니다.\n",
    "- 딥러닝을 사용한 keras에서 text classification에서 가장 많이 나오는 게 IMDB 분류!\n",
    "- IMDB : 영문판 NSMC, 영화 리뷰를 긍정/부정(1/0)을 분류\n",
    "- SVM을 사용하지않고 딥러닝 방법 중 딥러닝에서 많이 쓰이는 LSTM을 사용해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['짜증', '목소리'], '0']\n",
      "[['평점'], '0']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "from konlpy.tag import Okt\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "okt = Okt()\n",
    "mecab = Mecab(dicpath=\"C:\\\\mecab\\\\mecab-ko-dic\")\n",
    "\n",
    "if os.path.exists('train_docs.json'):\n",
    "    with open(\"train_docs.json\", encoding='utf-8') as f:\n",
    "        train_data = json.load(f)\n",
    "else:\n",
    "    train_data = [(text_tokenizing(text_cleaning(line[1])), line[2]) for line in train_docs if text_tokenizing(line[1])]\n",
    "    #train_data = [(text_tokenizing(line[1]), line[2]) for line in train_docs if text_tokenizing(line[1])]\n",
    "    \n",
    "    with open(\"train_docs.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(train_data, f, ensure_ascii=False, indent='\\t')\n",
    "        \n",
    "if os.path.exists('test_docs.json'):\n",
    "    with open(\"test_docs.json\", encoding='utf-8') as f:\n",
    "        test_data = json.load(f)\n",
    "else:\n",
    "    test_data = [(text_tokenizing(text_cleaning(line[1])), line[2]) for line in test_docs if text_tokenizing(line[1])]\n",
    "    #test_data = [(text_tokenizing(line[1]), line[2]) for line in test_docs if text_tokenizing(line[1])]\n",
    "    with open(\"test_docs.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(test_data, f, ensure_ascii=False, indent='\\t')\n",
    "\n",
    "pprint(train_data[0])\n",
    "pprint(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['짜증', '목소리'], '0'], [['포스터', '초딩', '영화', '오버', '연기'], '1'], [['교도소', '이야기', '재미', '평점', '조정'], '0']]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network로 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 불러오기\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential # keras에서 모델 만들 때 sequential object를 만들고 여기에 하나씩 layer를 추가하는 방식(강사님은 쓰지 않음)\n",
    "# 네트워크 코드를 짜는 방법이 여러가지가 있는데 그중에 sequential방법이 있음\n",
    "# 강사님은 앞선 코드처럼 하려고하기때문에 sequential말고 다른 방식(layer마다 하나하나씩) 쓴다고함\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "# 실제로 RNN과 LSTM구조는 cell 단위로 weight가 넘어가는데\n",
    "# text는 길이가 다르기때문에 길이 다른걸 체크해주려면 최대길이를 설정할건데 최대길이보다 text가 짧을 수도 있기때문에\n",
    "# 나머지 길이를 채워야할 때 자동으로 채워주는게 pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameter \n",
    "- Hyperparameter는 우리가 마음대로 정하면 되지만 강사님 추천은 1~2만개 이상 쓰는 게 성능이 높았대\n",
    "    - 5만개 이상 넘어가면 너무 오래 걸렸대"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 35000 # feature selection 방법 (단어가 16만개정도 있는데 이걸 feature로 모두 사용하면 너무 많으니 불필요한 데이터는 자르자는 측면)\n",
    "max_len = 30 # 문서의 최대 길이\n",
    "batch_size = 128 # 보통 32의 배수로 많이 씀\n",
    "EPOCHS = 4 # 높을 수록 성능이 잘 나오는데 오래걸림. 실습을 위해 4번정도만 (너무 높게 잡으면 overfitting일어남. 30정도가 적당했대)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network Structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__() # 상속받은 keras 모델의 init을 불러와\n",
    "    # __init__에 네트워크 구조를 순서대로 짠다고 생각해\n",
    "    # sequence로 짤 수가 있고, layer마다 하나하나 달아서 쓸 수도 있음\n",
    "        self.emb = Embedding(max_words, 100) # 하나하나의 node(LSTM 전체 구조)가 100차원짜리고 max_words 개수만큼 최대로 가질 수 있음\n",
    "        self.lstm = LSTM(128, dropout=0.2, recurrent_dropout=0.2) # dropout 쓰면 overfitting이 좀 줄어듦(reguralization 방법)\n",
    "        self.dense = Dense(1, activation='sigmoid') # 여기서 Dense layer는 1개 (0 또는 1 출력)\n",
    "        \n",
    "    def __call__(self, x, training=None, mask=None):\n",
    "        # 불러와서 쓸 때\n",
    "        x = self.emb(x)\n",
    "        x = self.lstm(x)\n",
    "        return self.dense(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<우리가 만들어야하는 형태>**</br></br>\n",
    "- 토큰을 전부 이름으로 바꿔줘(컴퓨터는 text를 못 알아먹으니까 단어를 1:1로 숫자로 맵핑한 값)\n",
    "- 최대 30개로 zero-padding해줘(앞에서부터 0으로 채우면서)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48, 222]\n",
      "\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  48 222]\n"
     ]
    }
   ],
   "source": [
    "# SVM 때와 비슷한 형태로 만들고\n",
    "x_train = [doc for doc, _ in train_data]\n",
    "\n",
    "# keras가 사용하기 위한 형태로 Tokenizing\n",
    "# text tokenizer를 쓰기 위해서 기준이 되는 데이터가 먼저 필요해서 x_train 먼저 선언해줬음\n",
    "tokenizer = Tokenizer(num_words=max_words) # 최대 몇개의 단어를 불러올 건지(max_words로 정의해줬었지)\n",
    "tokenizer.fit_on_texts(x_train) # tokenizer쓸 때 꼭 fit을 해줘야 해\n",
    "\n",
    "# LSTM의 input으로 넣기 위한 변환 작업\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences([doc for doc, _ in test_data])\n",
    "y_train = np.array([int(label) for _, label in train_data])\n",
    "y_test = np.array([int(label) for _, label in test_data])\n",
    "print(x_train[0])\n",
    "# 실제로 뒤에서 해보면 숫자 타입이 안 맞아서 np.array로 바꿔줘야해(정확히는 타입 변환은 np.int로 바꿔주는 건데 가장 심플한 방법이 np.array로)\n",
    "\n",
    "# 크기를 맞춰주기 위한 zero padding\n",
    "x_train = pad_sequences(x_train, value=0, padding='pre', maxlen=max_len)\n",
    "x_test = pad_sequences(x_test, value=0, padding='pre', maxlen=max_len)\n",
    "print('\\n', x_train[0])\n",
    "# value=0 : padding할 때 0으로 채워라(default값이긴 하지만 좀 더 명시해주기위해)\n",
    "# padding='pre' : 앞부터 채워라\n",
    "\n",
    "# 학습 가능한 형태로 최종 변환.\n",
    "# ds : data structure\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size) \n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "# 10000개씩 shuffle, test에서는 당연히 shuffle 필요 없겠지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[48, 222] -> 위에 ['짜증', '목소리']가 숫자로 바뀐 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 선언\n",
    "model = SimpleLSTM()\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "# loss가 중요! 아무거나 쓰면 절대 안돼\n",
    "# 심지어 layer가 어떤 것이고 어떻게 설정해줬냐에따라 다르게 setting을 해줘야해\n",
    "# 우리는 dense layer가 1이고 sigmoid 활성함수 이용 -> binary_crossentropy를 써야해(외워! 0과 1 두개의 class로 구분할 때 쓰는 것)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopper = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, verbose=1)\n",
    "# 실제로 validation loss가 쭉 내려가야하는데 중간에 내려가다가 안 내려가고 계속 올라가면 training이 overfitting 되고있다는 뜻\n",
    "# 이런 경우에 멈춰라 해주는 것! (메모리 낭비 방지)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1054 steps, validate for 352 steps\n",
      "Epoch 1/4\n",
      "1054/1054 [==============================] - 267s 254ms/step - loss: 0.5192 - accuracy: 0.7309 - val_loss: 0.4940 - val_accuracy: 0.7512\n",
      "Epoch 2/4\n",
      "1054/1054 [==============================] - 264s 251ms/step - loss: 0.4537 - accuracy: 0.7783 - val_loss: 0.5012 - val_accuracy: 0.7452\n",
      "Epoch 3/4\n",
      "1054/1054 [==============================] - 293s 278ms/step - loss: 0.4159 - accuracy: 0.7993 - val_loss: 0.5217 - val_accuracy: 0.7463\n",
      "Epoch 4/4\n",
      "1054/1054 [==============================] - 250s 238ms/step - loss: 0.3806 - accuracy: 0.8168 - val_loss: 0.5660 - val_accuracy: 0.7400\n"
     ]
    }
   ],
   "source": [
    "# 실행, 결과 저장.\n",
    "histopy = model.fit(train_ds, validation_data=test_ds, epochs=EPOCHS, callbacks=[earlystopper])\n",
    "\n",
    "# Epoch가 진행됨에따라 loss는 떨어지고 ETA(accuracy)는 올라가는 게 정상!\n",
    "# 만약 초반 Epoch부터 loss가 올라가고 accuracy가 떨어진다면 underfitting되고 있다는 뜻(모델 잘못 만듦) -> 빠르게 멈춰서 모델 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45034/45034 [==============================] - 9s 206us/sample - loss: 0.5660 - accuracy: 0.7400\n"
     ]
    }
   ],
   "source": [
    "# Model Test\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score:  0.5659710426003809\n",
      "Test Accuracy:  0.74004084\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Score: \", score)\n",
    "print(\"Test Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'History' object has no attribute 'histopy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-254bed897e0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistopy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'History' object has no attribute 'histopy'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "loss = histopy.histopy['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss, 'ro-', label=\"train_loss\")\n",
    "plt.plot(val_loss, 'bo-', label=\"val_loss\")\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "# train_loss는 쭉 떨어지고있고 val_loss는 올라가는 중\n",
    "# val_loss가 올라가고있다는 건 기본적으로 over_fitting되고있다는 뜻\n",
    "# 지금은 Epoch가 너무 짧아서 올라갔다가 내려가는지, 아님 계속 올라가는지 확인 불가능\n",
    "# 실제로 Epoch 늘려보면 overfitting이 되고있지는 않대"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model weight matrix 저장.\n",
    "model.save_weights(\"nsmc_keras_simplelstm\")\n",
    "# 나중에 불러올 때는 model.load_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN 학습 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # tensorflow 중 keras 이용\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 구조 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyModel(): # 함수화 해줬음. 지금까진 tensorflow 2.0 방식으로만 실습하고있어서\n",
    "    # keras 방식으로도 알려줄려고 이번엔 함수화 했음\n",
    "        return Sequential([Conv2D(32, (3,3), padding='same', activation='relu'), # 28x28x32의 feature map 얻음\n",
    "                           MaxPool2D(), # 14x14x32\n",
    "                           Conv2D(64, (3,3), padding='same', activation='relu'), # 14x14x64\n",
    "                           MaxPool2D(), # 7x7x64\n",
    "                           Conv2D(128, (3,3), padding='same', activation='relu'), # 7x7x128\n",
    "                           Flatten(), # 7x7x64 = 6272 \n",
    "                           Dense(128, activation='relu'), # 128로 줄여줘\n",
    "                           Dense(10, activation='softmax')]) # fashion mnist는 마지막에 출력이 10개니까\n",
    "                                           # 32, 64, 128, 256채널 \n",
    "        # Sequential안에 리스트로 사용할 layer들 다 넣어주면 돼\n",
    "        # Conv2D(필터개수, 커널 size, )\n",
    "        # padding='valid': zero-padding 적용x , padding='same': zero-padding 적용, 영상크기 그대로 유지\n",
    "        # Maxpool2D()는 안에 아무것도 안 넣어주면 알아서 2x2로 작동함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n",
    "\n",
    "# NHWC\n",
    "x_train = x_train[..., np.newaxis]\n",
    "x_test = x_test[..., np.newaxis]\n",
    "# ...은 앞에 있는 모든 axis에 대해 전부 다 포함한다는 뜻\n",
    "# np.newaxis는 그 뒤에 axis를 하나 더 붙여준다는 뜻\n",
    "# 즉, 모든 axis를 포함해서 뒤에 axis 하나 더 붙여준다\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32).prefetch(2048)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32).prefetch(2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape) # (batch 개수, height, width) \n",
    "print(x_train[0].shape)\n",
    "\n",
    "# 3차원으로 되어있는 데이터를 차원을 하나 더 붙여줘야 돼\n",
    "# CNN 학습할 때 데이터 셋의 구조가 rank=4인 NHWC(batch, height, width, channel)를 필요로 하기때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train[0].shape)\n",
    "\n",
    "# 4차원 데이터로 바뀌었지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1875 steps, validate for 313 steps\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 132s 70ms/step - loss: 0.3838 - accuracy: 0.8599 - val_loss: 0.3205 - val_accuracy: 0.8832\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 115s 61ms/step - loss: 0.2457 - accuracy: 0.9106 - val_loss: 0.2459 - val_accuracy: 0.9098\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 139s 74ms/step - loss: 0.2034 - accuracy: 0.9241 - val_loss: 0.2350 - val_accuracy: 0.9153\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 144s 77ms/step - loss: 0.1694 - accuracy: 0.9361 - val_loss: 0.2297 - val_accuracy: 0.9190\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 111s 59ms/step - loss: 0.1429 - accuracy: 0.9460 - val_loss: 0.2394 - val_accuracy: 0.9192\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 104s 55ms/step - loss: 0.1174 - accuracy: 0.9556 - val_loss: 0.2692 - val_accuracy: 0.9170\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 91s 49ms/step - loss: 0.0945 - accuracy: 0.9646 - val_loss: 0.2796 - val_accuracy: 0.9186\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 106s 57ms/step - loss: 0.0755 - accuracy: 0.9712 - val_loss: 0.2783 - val_accuracy: 0.9227\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 130s 69ms/step - loss: 0.0645 - accuracy: 0.9755 - val_loss: 0.3102 - val_accuracy: 0.9210\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 111s 59ms/step - loss: 0.0514 - accuracy: 0.9806 - val_loss: 0.3778 - val_accuracy: 0.9169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x241988acc88>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, validation_data=test_ds, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 우리가 사용하던 FC layer만으로 돼있는 DNN보다 성능 더 좋음\n",
    "- **training loss는 떨어지는데 validation loss는 올라간다는 것은 overfitting이 발생하고있다는 것**\n",
    "- overfitting 방지하는 방법배웠었지\n",
    "    - early stopping도 해보고, dense에 drop out도 넣어서 해보고, 모델이 너무 깊은 게 아닌가 해서 모델 줄여서도 해보자\n",
    "- keras의 Sequential을 이용해 짧은 구현할 때 심플하고 빠르게 구현가능 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

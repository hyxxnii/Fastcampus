{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "\n",
    "> the collective name for a set of language modeling and feature learning techniques in NLP where words and phrases from the vocabulary are mapped to vectors of real numbers. (from Wikipedia)\n",
    "\n",
    "> 수학적으로, 고차원의 공간을 더 낮은 공간으로 변환하는 방법(embedding)과 같은 의미이기도 하다.\n",
    "\n",
    "> 결국, 고차원으로 표현된 feature vector(local representation, BOW, TF-IDF 등)을 distributional semantic을 가지는 vector space에 mapping 시켜주는 방법이다.\n",
    "\n",
    "> <b>\"You shall know a word by the company it keeps\"(John R. Firth, 1957)<b>, it called \"Distributed Hypothesis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![word embedding fig](7.PNG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visualize word vectors](8.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 최근 많이 쓰이는 word embedding 방법들이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wevi : word embedding visual inspector\n",
    "    \n",
    "> https://ronxin.github.io/wevi/\n",
    "\n",
    "이론을 정리하기 위해 체험을 해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 현재 word embedding이 핫하게 된 시작 알고리즘. \"Distributed representations of words and phrases and their compositionality(NIPS 2013)\" 에 처음 소개되었다.\n",
    "\n",
    "> Reference : https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![skip-gram](9.PNG)\n",
    "![simple-skip-gram](10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Skip Gram : 어떤 target 단어(학습을 하고 싶은 단어)와 주변 단어(context word)를 설정해서 함께 등장하는 단어들이 있을 텐데, 정말 함께 등장 했는지 안 했는지를 binary classification으로 해결하는 것\n",
    "- 이 단어와 함께 나오는 단어들이 얼마나 높은 확률로 등장하는 지 학습하는 것\n",
    "- 같이 등장하는 것을 positive sample, 같이 등장하지 않는 것을 negative sample이라고 정의\n",
    "   \n",
    "> Skip Gram할 때 윈도우라는 개념있음.\n",
    "- 윈도우: 문장에 단어들이 쭉 있으면 이 단어들을 내가 한번에 눈으로 보는 단위 정도\n",
    "- 문장을 쭉 볼 때 맨 앞부터 맨 뒤까지 신경쓰면서 읽진 않지. 문맥이라는 걸 파악하는 어느정도의 단위를 윈도우라고 정의함. 함께 나온다/안 나온다를 정의 하는 대상의 규모가 윈도우가 됨\n",
    "- 윈도우 안에 있으면 함께 나옴 / 윈도우가 밖에 있으면 함께 안 나옴\n",
    "- 윈도우 크기는 사용자가 정의해줘야하고, 크기에 따라 문맥이라는 걸 파악하는 길이가 짧아질 수도 있고 길어질 수도 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Skip-Gram with Negative Sampling, 줄여서 SGNS라고 부르며 Neural Net을 이용한 word embedding이 빠르게 구현가능해진 이유기도 하다.\n",
    "\n",
    "> Negative Sampling이란, 마지막 단계의 softmax를 구하는 문제를 주변 단어(postive class)와 무작위로 골라진 나머지 단어들(negative class)로 분류하는 binary classfication 문제로 바꿔주는 기법이며, 이를 통해 굉장히 빠르게 word embedding 수행이 가능하다.\n",
    "- Negative Sampling 방법은 계산량을 많이 줄여줌. 강의자료에 나왔던 nplm이라는 개념은 softmax를 해서 이 단어가 빈칸에 등장할 확률을 max가 되게끔 학습을 해주는데 이게 단어 전체를 softmax 계산해야해서 굉장히 오래걸려\n",
    "- 이 친구는 softmax 계산을 매번 다 안하고 Negative Sampling이라는 방법을 사용해서 이 단어들이 동시 등장했다면 두 단어들은 가까우니 두 단어의 dot product(내적)값이 커지게, 서로가 변환공간에서 가까워지게끔 학습\n",
    "- 두 단어가 Negative Sampling이면 멀어지게끔 loss function 정의해서 학습하고 loss function이 최소가 되게끔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\영현\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\영현\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews') # 간단한 영문 데이터 셋\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot',\n",
       " ':',\n",
       " 'two',\n",
       " 'teen',\n",
       " 'couples',\n",
       " 'go',\n",
       " 'to',\n",
       " 'a',\n",
       " 'church',\n",
       " 'party',\n",
       " ',',\n",
       " 'drink',\n",
       " 'and',\n",
       " 'then',\n",
       " 'drive',\n",
       " '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "sentences = [list(sent) for sent in movie_reviews.sents()]\n",
    "sentences[0] # input이 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim에서 word2vec 불러오기\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# gensim (generator similar) : 토픽 모델링를 위해 처음 등장했지만, 지금은 word2vec이나 fasttext같은 걸로 훨씬 많이 쓰임\n",
    "# 지금은 자연어처리의 보명적인 library가 되기위해 여러가지 구현되었음\n",
    "# 특히 word embedding 파트에서 강력하게 이것저것 구현되어있음\n",
    "\n",
    "# 학습 모델 구현.\n",
    "\n",
    "# Word2Vec의 data input 형태는 list of list of word\n",
    "# 토큰 단위로 되어있는 리스트 1개가 문장이고, 이 문장들이 들어있는 list가 input이 됨\n",
    "w2v_model = Word2Vec(sentences, min_count=5, size=300, sg=1, iter=10,\n",
    "                     workers=4, ns_exponent=0.75, window=7)\n",
    "# min_count=5: 최소단어가 5개 이하면 쓰지 않겠다\n",
    "# size : hidden layer의 노드 개수\n",
    "# sg=1 : Skip Gram 쓰겠다\n",
    "# iter : iteration이 많으면 많을 수록 좋겠지만 지금은 간단하게\n",
    "# workers : 실제 CPU를 몇개 쓸거냐\n",
    "# window=7 : 내 기준으로 앞,뒤 7개를 보겠다 => 나 기준 왼쪽으로 3개 오른쪽으로 3개를 동시에 보겠다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('decent', 0.525941014289856),\n",
       " ('lousy', 0.4812678098678589),\n",
       " ('pitiful', 0.46957242488861084),\n",
       " ('passable', 0.46780040860176086),\n",
       " ('glorified', 0.4646782875061035),\n",
       " ('darned', 0.4630981981754303),\n",
       " ('gutsy', 0.46071916818618774),\n",
       " ('milestone', 0.458088755607605),\n",
       " ('commendable', 0.45794373750686646),\n",
       " ('dopey', 0.45742207765579224)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 평가, 비슷한 단어 찾기.\n",
    "w2v_model.wv.most_similar('good') \n",
    "# word2vec가 학습이 다 되면 embedding 공간이 생기고, 그 embeddin 공간에 있는 어떤 단어와 가장 비슷한 단어 찾아줌\n",
    "# 비슷하다는 기준은 cosine similarity가 됨\n",
    "# word2vec는 애초에 학습할 때 어떤 단어가 가깝다라는 걸 cosine similarity를 가정하기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<학습결과> - good이랑 관련없는 단어들이 나왔음. 왜 그런가 이유가 가장 중요\n",
    "</br>\n",
    "- word2vec는 비슷하게 같이 등장하면 얘네들의 학습을 높게 해줌\n",
    "- 감정표현이 나오는 문장에서 예를 들어, i am good 이라고 하면 i am ___ 이라는 건데, 그 빈칸 앞 뒤가 비슷했다는거지 (i am bad, i am decent) (아까 학습원리를 되돌아보면)\n",
    "- good과 비슷한 곳에 형용사가 등장했다는 것을 알 수 있고, 기분을 나타내는 형용사가 나왔는데 good과 비슷한 단어는 나오질 않았지\n",
    "- 하이퍼파라미터 조정을 좀 더 해봐야하고, good처럼 긍정적인 단어가 많이 들어있는 corpus를 더 넣어준다는 등의 전처리 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> GloVe는 Global Vectors의 약자로, aggregated global word global co-occurrence statistics를 최적화하는 방향으로 학습하는 word embedding 방법이다. \"GloVe: Gloval Vectors for Word Representation(EMNLP 2014)\"에 소개되었다.\n",
    "\n",
    "> Reference : https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "- word2vec는 윈도우 크기 내에 있는 단어들만 확률적으로 높게 계산되게끔 학습이 된다고 했는데, 그 말은 local 정보를 사용하겠다는 것(Global 정보 빠져있음)\n",
    "- 그래서 GloVe는 Global 정보를 사용하겠다고해서 word에 Global co-occurence의 등장 확률 값이 높게끔 학습\n",
    "- 밑의 식이 loss function\n",
    "- 밑의 metrix 값과 동시등장하는 두 단어의 dot product값이 서로 비슷해지게, 내가 지금 보고있는 target 단어와 주변 단어가 global co-occurrence값이 높으면 이 두 단어의 dot product가 행렬 분해한 값과 비슷해지게 끔 학습 <= 밑의 식의 해석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![glove](11.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe는 현재 파이썬으로는 구현이 힘듭니다(업데이트가 안돼서)\n",
    "# 사용을 위해서는 c++을 사용하셔야 합니다.\n",
    "\n",
    "#from glove import Glove\n",
    "\n",
    "#glove_model = Glove(no_components=100, learning_rate=0.05)\n",
    "#glove_model.fit(sentences, epochs=10, no_threads=4, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 현재 NLP task에서 word embedding의 baseline으로 사용되는 기법이다. subword embedding model, char n-gram embedding model이라고도 한다.\n",
    "\n",
    "> word2vec을 만들었던, Tomas Mikolov가 Google에서 Facebook으로 옮긴 뒤에 낸 모델로 word2vec의 단점을 보완한 모델이다.\n",
    "\n",
    "> word2vec의 단점이었던, OOV(Out Of Vocabulary) 문제와 low frequency를 많이 해결하였다.\n",
    "- OOV : 학습 corpus에 없었던 단어는 word2vec로 표현이 안되는 문제(반영하려면 다시 전체 학습을 시켜줘야해서 복잡했음)\n",
    "- low frequency: word2vec는 근본적으로 positive/negative sampling 방법을 사용할 때 단어의 등장 횟수 자체가 적으면 무시되는 경향이 컸음\n",
    "\n",
    "> word를 subword 단위로 표현하는 것으로 기본적으로 SGNS 방식이다.\n",
    "\n",
    "> Reference : https://fasttext.cc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![char3-grams](12.png)\n",
    "![char3-grams](13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> characet N-grams 이라는 방법 사용하게됨\n",
    "- going이라는 단어를 N=3을 적용, <go와 ng>에 나오는 꺽쇠(<,>)는 단어의 시작과 끝을 알림 (go와 ng라는 단어가 중간에 등장할 수도 있기때문에 차이를 주기위해서)\n",
    "- 그래서 going에 <,>을 추가해서 7개를 연달아 3개씩 나눠\n",
    "- 나눠진 각 sub component는 +라는 concatnate 연산돼서 going이라는 벡터에 가까워지게끔 학습됨\n",
    "- 물론 각 sub component 토큰들은 다른 단어에서도 쓰이겠지. 그러면 거기에도 가까워지게끔 매번 학습됨\n",
    "- 그래서 각 sub component들이 본래의 의미를 가질 수 있게끔\n",
    "\n",
    "> 실제 한글의 특징 때문에 한글에서 많이 쓰임. 한글의 자소 분리(초/중/종성)를 n-grams으로 짤라서 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim에서 FastText 불러오기\n",
    "from gensim.models import FastText\n",
    "\n",
    "# FastText 학습.\n",
    "fast_model = FastText(sentences, min_count=5, sg=1, size=300, workers=4,\n",
    "                      min_n=2, max_n=7, alpha=0.05, window=7, iter=10)\n",
    "# min_n/max_n : character n-gram을 실제로 할 땐 n을 고정시키는 게 아니고 최대/최소 개수를 주고 그것들의 ramdon sampling을 하게끔 만듦\n",
    "# 이러면 다양한 학습이 될 수 있고, 똑같은 단어에서 정보를 많이 뽑아낼 수 있음\n",
    "\n",
    "fast_model.save('fast_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('goods', 0.5055395364761353),\n",
       " ('goodnight', 0.43297654390335083),\n",
       " ('great', 0.41480714082717896),\n",
       " ('goofball', 0.40592318773269653),\n",
       " ('goose', 0.39415624737739563),\n",
       " ('bad', 0.3819223642349243),\n",
       " ('marvellous', 0.36979711055755615),\n",
       " ('goo', 0.35548409819602966),\n",
       " ('gutsy', 0.34930139780044556),\n",
       " ('glorified', 0.3475170135498047)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결과 평가.\n",
    "fast_model.wv.most_similar('good') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<학습결과> - 아까랑 다르게 good이랑 관련된 단어들이 많이 나왔음\n",
    "</br>\n",
    "- good이라는 sub word들을 학습했기때문에 good의 sub component들이 있는 단어들의 학습이 잘 되겠지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ELMo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ELMo는 Embeddings from Language Model의 약자입니다. ELMo는 **pre-trained language model**을 사용하여 문맥에 맞는 word embedding, \"Contextualized Word Embedding\"을 만드는 방법입니다.\n",
    "- 사용자가 어떤 주어진 데이터에 대해서 학습하는 데이터에 따라 다른 맥락들을 가지고 학습\n",
    "- fasttext, word2vec, glove는 사실 통계 정보(동시등장확률)만 썼었는데, ELMo는 동시등장을 좀 더 다르게 학습하겠다, Contextual하게 학습하겠다라는 것\n",
    "\n",
    "> bidirectional Language Model을 이용하여, pre-trained embedding vector를 corpus의 context(syntax, semantics, polysemy) 정보를 보완해주는 embedding vector를 만들어 준다.\n",
    "\n",
    "> tensorflow, pytorch를 통해서 bidirectional LSTM model을 만들어 사용이 가능하다. (이미 구현된 model이 github에 공개되어있다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"Deep contextualized word representations(NAACL 2018)\"에 소개된 방법입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_hub\n",
      "  Downloading tensorflow_hub-0.8.0-py2.py3-none-any.whl (101 kB)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\users\\hyun\\lib\\site-packages (from tensorflow_hub) (3.11.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hyun\\lib\\site-packages (from tensorflow_hub) (1.13.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\hyun\\lib\\site-packages (from tensorflow_hub) (1.17.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hyun\\lib\\site-packages (from protobuf>=3.8.0->tensorflow_hub) (42.0.2)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.8.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Exporting/importing meta graphs is not supported when eager execution is enabled. No graph exists when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-642cc6aabd45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 가까운 미래에는 실행이 되길 빌면서..\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0melmo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/google/elmo/3'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m embeddings = elmo(\n",
      "\u001b[1;32mC:\\Users\\hyun\\lib\\site-packages\\tensorflow_hub\\module.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, spec, trainable, name, tags)\u001b[0m\n\u001b[0;32m    174\u001b[0m           \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m           tags=self._tags)\n\u001b[0m\u001b[0;32m    177\u001b[0m       \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\hyun\\lib\\site-packages\\tensorflow_hub\\native_module.py\u001b[0m in \u001b[0;36m_create_impl\u001b[1;34m(self, name, trainable, tags)\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkpoint_variables_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m         name=name)\n\u001b[0m\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_export\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables_saver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\hyun\\lib\\site-packages\\tensorflow_hub\\native_module.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, spec, meta_graph, trainable, checkpoint_path, name)\u001b[0m\n\u001b[0;32m    443\u001b[0m     \u001b[1;31m# TPU training code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mscope_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_init_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\hyun\\lib\\site-packages\\tensorflow_hub\\native_module.py\u001b[0m in \u001b[0;36m_init_state\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_init_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mvariable_tensor_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_state_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m     self._variable_map = recover_partitioned_variable_map(\n\u001b[0;32m    450\u001b[0m         get_node_map_from_tensor_map(variable_tensor_map))\n",
      "\u001b[1;32mC:\\Users\\hyun\\lib\\site-packages\\tensorflow_hub\\native_module.py\u001b[0m in \u001b[0;36m_create_state_graph\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    503\u001b[0m         \u001b[0mmeta_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m         \u001b[0minput_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 505\u001b[1;33m         import_scope=relative_scope_name)\n\u001b[0m\u001b[0;32m    506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m     \u001b[1;31m# Build a list from the variable name in the module definition to the actual\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\training\\saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[1;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[0;32m   1451\u001b[0m   return _import_meta_graph_with_return_elements(meta_graph_or_file,\n\u001b[0;32m   1452\u001b[0m                                                  \u001b[0mclear_devices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimport_scope\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1453\u001b[1;33m                                                  **kwargs)[0]\n\u001b[0m\u001b[0;32m   1454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\training\\saver.py\u001b[0m in \u001b[0;36m_import_meta_graph_with_return_elements\u001b[1;34m(meta_graph_or_file, clear_devices, import_scope, return_elements, **kwargs)\u001b[0m\n\u001b[0;32m   1461\u001b[0m   \u001b[1;34m\"\"\"Import MetaGraph, and return both a saver and returned elements.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1462\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1463\u001b[1;33m     raise RuntimeError(\"Exporting/importing meta graphs is not supported when \"\n\u001b[0m\u001b[0;32m   1464\u001b[0m                        \u001b[1;34m\"eager execution is enabled. No graph exists when eager \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1465\u001b[0m                        \"execution is enabled.\")\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Exporting/importing meta graphs is not supported when eager execution is enabled. No graph exists when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "# tensorflow_hub에서 데이터 불러오기.\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# 가까운 미래에는 실행이 되길 빌면서..\n",
    "elmo = hub.Module('https://tfhub.dev/google/elmo/3', trainable=True)\n",
    "\n",
    "embeddings = elmo(\n",
    "    [\"the cat is on the mat\", \"dogs are in the fog\"],\n",
    "    signature=\"default\",\n",
    "    as_dict=True)[\"elmo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf 2.0 버전에는 ELMo가 안 들어가있음\n",
    "- tf 1.x 에서 실행하면 ELMo 쓸 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"module_apply_default/aggregation/mul_3:0\", shape=(2, 6, 1024), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reference : https://allennlp.org/elmo, https://github.com/allenai/bilm-tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ELMo는 LSTM이 2개가 들어있음. 순방향 1개, 역방향 1개로 붙어있음\n",
    "- 처음에 ELMo로 embedding 벡터들이 쭉 들어오는데 여기있는 embedding 벡터들은 앞에서 봤던 word2vec나 fasttext가 아니고 그냥 글자 하나하나(유니코드값으로 변환된 게 input으로 들어옴)\n",
    "- forward network 쪽에는 순서대로 들어오고, backward network 쪽에는 반대로 들어옴. 말 그대로 문장이라는 것을 앞에서 한번, 뒤에서 한번 양쪽방향으로 문맥을 학습하기위해. 그래서 LSTM 2개가 따로따로 학습된 다음에 그 결과들을 ELMo 임베딩이라는 걸로 맨 위에서 합쳐주는 것으로 실제 결과 임베딩 값이 나오게 됨\n",
    "- LSTM도 여러 개 layer라서 각 layer에 따라 각 단어들이 얼마나 가중치를 가지는지 학습됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![elmo_architecture](14.png)\n",
    "![elmo_architecture](15.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

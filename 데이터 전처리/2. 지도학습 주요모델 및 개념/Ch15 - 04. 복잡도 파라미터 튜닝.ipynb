{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 경로 설정 및 데이터 불러오기\n",
    "import os\n",
    "os.chdir('./data')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"Sonar_Mines_Rocks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특징과 라벨 분리\n",
    "X = df.drop('Y', axis = 1)\n",
    "Y = df['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터와 평가 데이터 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "Train_X, Test_X, Train_Y, Test_Y = train_test_split(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156, 60)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_X.shape # 샘플 156개, 특징 60개 => 단순한 모델 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M    87\n",
       "R    69\n",
       "Name: Y, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Y.replace({\"M\":-1, \"R\":1}, inplace = True)\n",
    "Test_Y.replace({\"M\":-1, \"R\":1}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1. 복잡도 파라미터가 한 개이면서, 단순하고, 우연성이 어느정도 있는 모델(경사 하강법 사용) - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_model_test(C):\n",
    "    model = LR(C = C, max_iter = 100000, random_state = 10).fit(Train_X, Train_Y) # 가벼운 모델이므로 max_iter를 크게 잡음\n",
    "    pred_Y = model.predict(Test_X)\n",
    "    return f1_score(Test_Y, pred_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.1:\t0.5581395348837209\n",
      "C = 1:\t0.7407407407407408\n",
      "C = 5:\t0.7169811320754718\n"
     ]
    }
   ],
   "source": [
    "print(\"C = 0.1:\\t{}\".format(LR_model_test(C = 0.1)))\n",
    "print(\"C = 1:\\t{}\".format(LR_model_test(C = 1))) \n",
    "print(\"C = 5:\\t{}\".format(LR_model_test(C = 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 튜닝 범위: 0.1 < C < 5\n",
    "- 아직 확정짓기에는 범위가 넓다 => 더 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.5:\t0.6923076923076924\n",
      "C = 2:\t0.7407407407407408\n"
     ]
    }
   ],
   "source": [
    "print(\"C = 0.5:\\t{}\".format(LR_model_test(C = 0.5)))\n",
    "print(\"C = 2:\\t{}\".format(LR_model_test(C = 2)))\n",
    "\n",
    "# 튜닝 범위: 0.1 < C < 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- C=2인 구간을 굳이 안 봐도 돼\n",
    "- 최종 탐색 구간: 0.1과 1 사이 (0.1과 0.5 사이도 몰라)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.7061224489795919, 'max_iter': 100000, 'random_state': 10} 0.7407407407407408\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 그리드 설정\n",
    "LR_parameter_grid = ParameterGrid({\"C\":np.linspace(0.1, 1, 50),\n",
    "                                  \"max_iter\":[100000],\n",
    "                                  \"random_state\":[10]})\n",
    "\n",
    "# 파라미터 튜닝 수행 \n",
    "best_score = -1\n",
    "for parameter in LR_parameter_grid:\n",
    "    model = LR(**parameter).fit(Train_X, Train_Y)\n",
    "    pred_Y = model.predict(Test_X)\n",
    "    score = f1_score(Test_Y, pred_Y)\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_parameter = parameter\n",
    "\n",
    "print(best_parameter, best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2. 복잡도 파라미터가 두 개이면서, 단순하고, 우연성이 거의 없는 모델 (Decision Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DTC_model_test(max_depth, min_samples_leaf):\n",
    "    model = DTC(max_depth = max_depth, min_samples_leaf = min_samples_leaf).fit(Train_X, Train_Y) \n",
    "    pred_Y = model.predict(Test_X)\n",
    "    return f1_score(Test_Y, pred_Y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-1:0.7083333333333333\n",
      "3-2:0.7083333333333333\n",
      "3-3:0.7083333333333333\n",
      "6-1:0.7843137254901961\n",
      "6-2:0.7083333333333333\n",
      "6-3:0.7692307692307692\n",
      "9-1:0.7307692307692307\n",
      "9-2:0.8\n",
      "9-3:0.7547169811320756\n"
     ]
    }
   ],
   "source": [
    "for max_depth in [3, 6, 9]:\n",
    "    for min_samples_leaf in [1, 2, 3]:\n",
    "        score = DTC_model_test(max_depth = max_depth, min_samples_leaf = min_samples_leaf)\n",
    "        print(\"{}-{}:{}\".format(max_depth, min_samples_leaf, score))\n",
    "\n",
    "# max depth가 크고(복잡도 증가) min_samples_leaf가 큰 경우(복잡도 감소) 좋은 성능이 나옴을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 8, 'min_samples_leaf': 2} 0.8\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 그리드 설정\n",
    "DTC_parameter_grid = ParameterGrid({\"max_depth\": np.arange(6, 15),\n",
    "                                  \"min_samples_leaf\": np.arange(2, 5)})\n",
    "\n",
    "# 파라미터 튜닝 수행 \n",
    "best_score = -1\n",
    "for parameter in DTC_parameter_grid:\n",
    "    model = DTC(**parameter).fit(Train_X, Train_Y)\n",
    "    pred_Y = model.predict(Test_X)\n",
    "    score = f1_score(Test_Y, pred_Y)\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_parameter = parameter\n",
    "\n",
    "print(best_parameter, best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 3. 복잡도 파라미터가 하나이면서, 우연성이 있는 모델 (신경망)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier as MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_model_test(hidden_layer_sizes):\n",
    "    model = MLP(hidden_layer_sizes = hidden_layer_sizes, random_state = 12).fit(Train_X, Train_Y) \n",
    "    pred_Y = model.predict(Test_X)\n",
    "    return f1_score(Test_Y, pred_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hyun\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\hyun\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,) 0.7000000000000001\n",
      "(10,) 0.6923076923076924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hyun\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\hyun\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3) 0.12903225806451613\n",
      "(5, 5) 0.0\n",
      "(10, 10) 0.7843137254901961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hyun\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# (5, ): 첫 번째 은닉층에 노드 5개\n",
    "# (3, 3): 첫 번째 은닉층에 노드 3개, 두 번째 은닉층에 노드 3개\n",
    "# 오른쪽으로 갈 수록 더 복잡한 형태\n",
    "for hidden_layer_sizes in [(5, ), (10, ), (3, 3), (5, 5), (10, 10)]:    \n",
    "    score = MLP_model_test(hidden_layer_sizes = hidden_layer_sizes)\n",
    "    print(hidden_layer_sizes, score)\n",
    "\n",
    "# max_iter warning 발생 \n",
    "# (5, 5)에서는 f1-score가 0이 나옴 \n",
    "#  => 초기값 영향(0을 찍고 올라오도록 초기값이 설정됨\n",
    "#  => random_state=12는 좋지 않음)으로 보여짐 (근거: 더 단순한 모델과 복잡한 모델에서는 성능이 나왔으므로)\n",
    "# (5, )와 (10, 10)에서 best_score가 나옴 => 더 복잡한 모델이 필요할지 판단이 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hyun\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\hyun\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\hyun\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\hyun\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\hyun\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\hyun\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\hyun\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\hyun\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\hyun\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\hyun\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\hyun\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\hyun\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layer_sizes': (10, 10), 'max_iter': 2000, 'random_state': 15} 0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 그리드 설정\n",
    "MLP_parameter_grid = ParameterGrid({\"random_state\": [41, 102, 15],\n",
    "                                  \"hidden_layer_sizes\": [(4, 4), (10, 10), (5, 5, 5), (10, 10, 10)],\n",
    "                                   \"max_iter\":[200, 2000, 20000]})\n",
    "\n",
    "# 파라미터 튜닝 수행 \n",
    "best_score = -1\n",
    "for parameter in MLP_parameter_grid:\n",
    "    model = MLP(**parameter).fit(Train_X, Train_Y)\n",
    "    pred_Y = model.predict(Test_X)\n",
    "    score = f1_score(Test_Y, pred_Y)\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_parameter = parameter\n",
    "\n",
    "print(best_parameter, best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 신경망을 튜닝할 때는 random_state, hidden_layer_sizes, max_iter를 같이 튜닝해야 함\n",
    "- 신경망이나 SVR은 하이퍼 파라미터를 잘 튜닝했을 때 정말 좋은 성능을 기대할 수 있는 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

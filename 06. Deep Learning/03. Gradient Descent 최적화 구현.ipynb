{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent 최적화 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실 함수 정의 (Analytic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 0.1*x**4 - 1.5*x**3 + 0.6*x**2 + 1.0*x + 20.0\n",
    "    # 이런 손실 함수를 알고있다고 가정하고 Gradient Descent 최적화 구현 해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실 함수 미분 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_dx(x):\n",
    "    return 0.4*x**3 - 4.5*x**2 + 1.2*x + 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 5 # 초기값\n",
    "eps = 1e-5 # gradient descent가 끝날 조건으로 입실론 정의 , 10의 -5승\n",
    "lr = 0.01 # learning rate\n",
    "max_epoch = 1000 # 1000번 돈다고 가정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent 알고리즘 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수가 가장 작게되는 것을 얻으려는 것\n",
    "min_x = x \n",
    "min_y = f(min_x)\n",
    "\n",
    "for _ in range(max_epoch):\n",
    "    # Gradient Descent의 정의 상 미분을 먼저 해야겠지\n",
    "    grad = df_dx(x) # 현재 위치는 x니까\n",
    "    new_x = x - lr * grad\n",
    "    y = f(new_x)\n",
    "    \n",
    "    # 이동된 것이 현재까지 얻은 mininum보다 더 minimum인지 아닌지 \n",
    "    if min_y > y: # update 해줘야겠지\n",
    "        min_x = new_x\n",
    "        min_y = y\n",
    "    \n",
    "    # 알고리즘을 종료해야할 때인지 아닌지 판단\n",
    "    # x가 변화하는 변화량이 입실론보다 작으면 종료\n",
    "    if np.abs(x - new_x) < eps:\n",
    "        break\n",
    "        \n",
    "    x = new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.955323272631201 -428.84677390087836\n"
     ]
    }
   ],
   "source": [
    "print(min_x, min_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한가지 확인해보자\n",
    "# 하이퍼파라미터로 x=5로 해주었는데 손실 함수가 있을 때 x=5부터 시작해서 gradient descent 했을 때\n",
    "# 가장 작게 나오는 것으로 추정된 것이 x=10.96, y=-428.85\n",
    "# 처음 시작한 x=5랑 가깝지\n",
    "# 시작위치가 달랐다고 해보자 -> x=-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## x = -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = -5 # 초기값\n",
    "eps = 1e-5 # gradient descent가 끝날 조건으로 입실론 정의 , 10의 -5승\n",
    "lr = 0.01 # learning rate\n",
    "max_epoch = 1000 # 1000번 돈다고 가정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수가 가장 작게되는 것을 얻으려는 것\n",
    "min_x = x \n",
    "min_y = f(min_x)\n",
    "\n",
    "for _ in range(max_epoch):\n",
    "    # Gradient Descent의 정의 상 미분을 먼저 해야겠지\n",
    "    grad = df_dx(x) # 현재 위치는 x니까\n",
    "    new_x = x - lr * grad\n",
    "    y = f(new_x)\n",
    "    \n",
    "    # 이동된 것이 현재까지 얻은 mininum보다 더 minimum인지 아닌지 \n",
    "    if min_y > y: # update 해줘야겠지\n",
    "        min_x = new_x\n",
    "        min_y = y\n",
    "    \n",
    "    # 알고리즘을 종료해야할 때인지 아닌지 판단\n",
    "    # x가 변화하는 변화량이 입실론보다 작으면 종료\n",
    "    if np.abs(x - new_x) < eps:\n",
    "        break\n",
    "        \n",
    "    x = new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.35277896788817237 19.789298288581236\n"
     ]
    }
   ],
   "source": [
    "print(min_x, min_y)\n",
    "# x=5때랑 전혀 다른 값이 나왔지\n",
    "\n",
    "# 처음 강의에서 말했던, gradient descent로 구한 정답은\n",
    "# local minimum인지 global minimum인지 장담할 수 없다고 했었던 그 의미\n",
    "\n",
    "# 이걸 어떻게 해결할 수 있을 까는 우리가 배웠던 진보된 알고리즘들\n",
    "# 특히 Adam이 사용될 것임\n",
    "# Adam이나 RMSProb같은 주요 알고리즘은 tensorflow에 구현되어있기때문에 그것을 앞으로 활용할 것임"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
